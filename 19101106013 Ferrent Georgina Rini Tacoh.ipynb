{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male-56.95%\n",
      "Male-80.50%\n",
      "Male-54.67%\n",
      "Male-82.41%\n",
      "Male-87.36%\n",
      "Male-84.96%\n",
      "Male-63.03%\n",
      "Male-82.55%\n",
      "Male-77.67%\n",
      "Male-71.69%\n",
      "Male-51.01%\n",
      "Female-88.13%\n",
      "Male-70.23%\n",
      "Female-86.94%\n",
      "Female-88.68%\n",
      "Female-65.15%\n",
      "Female-55.21%\n",
      "Female-77.92%\n",
      "Female-93.46%\n",
      "Female-84.10%\n",
      "Female-90.81%\n",
      "Female-81.34%\n",
      "Female-78.55%\n",
      "Female-76.37%\n",
      "Female-84.81%\n",
      "Female-56.65%\n",
      "Male-62.71%\n",
      "Female-59.51%\n",
      "Male-62.80%\n",
      "Male-73.97%\n",
      "Male-76.38%\n",
      "Male-83.33%\n",
      "Male-91.16%\n",
      "Male-86.81%\n",
      "Male-86.37%\n",
      "Male-88.08%\n",
      "Male-85.07%\n",
      "Male-88.12%\n",
      "Male-81.11%\n",
      "Male-82.43%\n",
      "Male-77.01%\n",
      "Male-75.14%\n",
      "Male-88.09%\n",
      "Male-73.80%\n",
      "Male-87.47%\n",
      "Male-70.52%\n",
      "Male-69.98%\n",
      "Male-70.10%\n",
      "Male-59.60%\n",
      "Male-69.60%\n",
      "Male-70.14%\n",
      "Male-60.61%\n",
      "Male-63.71%\n",
      "Male-69.08%\n",
      "Male-74.55%\n",
      "Male-52.33%\n",
      "Male-67.29%\n",
      "Male-68.38%\n",
      "Male-77.96%\n",
      "Male-56.14%\n",
      "Female-60.41%\n",
      "Female-73.88%\n",
      "Male-63.92%\n",
      "Male-73.88%\n",
      "Male-52.04%\n",
      "Male-69.71%\n",
      "Male-63.66%\n",
      "Male-54.91%\n",
      "Male-65.36%\n",
      "Female-51.94%\n",
      "Male-54.76%\n",
      "Male-57.93%\n",
      "Female-73.74%\n",
      "Male-53.10%\n",
      "Female-52.71%\n",
      "Female-55.69%\n",
      "Male-53.82%\n",
      "Female-56.45%\n",
      "Male-61.41%\n",
      "Male-75.29%\n",
      "Male-57.13%\n",
      "Male-63.90%\n",
      "Male-56.75%\n",
      "Male-72.68%\n",
      "Female-67.94%\n",
      "Male-72.79%\n",
      "Male-78.07%\n",
      "Male-83.97%\n",
      "Male-70.40%\n",
      "Male-69.93%\n",
      "Male-91.29%\n",
      "Male-78.59%\n",
      "Male-77.63%\n",
      "Male-50.43%\n",
      "Male-81.06%\n",
      "Male-88.96%\n",
      "Male-77.09%\n",
      "Male-74.30%\n",
      "Male-79.45%\n",
      "Male-90.87%\n",
      "Male-93.04%\n",
      "Male-72.39%\n",
      "Male-87.63%\n",
      "Male-94.67%\n",
      "Male-95.16%\n",
      "Male-78.59%\n",
      "Male-91.35%\n",
      "Male-97.43%\n",
      "Male-86.82%\n",
      "Male-71.56%\n",
      "Male-92.56%\n",
      "Male-66.79%\n",
      "Male-55.45%\n",
      "Male-72.27%\n",
      "Male-70.95%\n",
      "Male-75.61%\n",
      "Male-78.10%\n",
      "Male-69.09%\n",
      "Male-84.54%\n",
      "Male-77.92%\n",
      "Male-79.92%\n",
      "Male-86.07%\n",
      "Male-85.81%\n",
      "Male-50.16%\n",
      "Male-80.39%\n",
      "Male-70.13%\n",
      "Male-62.65%\n",
      "Male-84.90%\n",
      "Male-72.81%\n",
      "Male-66.32%\n",
      "Male-71.37%\n",
      "Male-67.93%\n",
      "Male-76.35%\n",
      "Male-68.21%\n",
      "Male-54.04%\n",
      "Male-82.55%\n",
      "Male-87.67%\n",
      "Male-68.54%\n",
      "Male-55.21%\n",
      "Male-61.95%\n",
      "Male-52.67%\n",
      "Male-67.43%\n",
      "Male-70.80%\n",
      "Male-62.42%\n",
      "Male-68.97%\n",
      "Male-62.98%\n",
      "Male-79.71%\n",
      "Female-89.25%\n",
      "Female-52.92%\n",
      "Female-94.27%\n",
      "Female-72.02%\n",
      "Female-85.10%\n",
      "Female-84.22%\n",
      "Female-97.61%\n",
      "Female-89.50%\n",
      "Female-82.68%\n",
      "Female-84.39%\n",
      "Female-95.09%\n",
      "Female-96.92%\n",
      "Female-96.73%\n",
      "Female-90.96%\n",
      "Female-88.36%\n",
      "Female-87.01%\n",
      "Female-86.14%\n",
      "Female-78.62%\n",
      "Female-79.30%\n",
      "Female-81.55%\n",
      "Female-76.90%\n",
      "Female-78.42%\n",
      "Female-87.43%\n",
      "Female-89.32%\n",
      "Female-86.18%\n",
      "Female-81.67%\n",
      "Female-86.69%\n",
      "Female-69.36%\n",
      "Female-79.03%\n",
      "Female-68.39%\n",
      "Female-75.46%\n",
      "Female-65.24%\n",
      "Female-65.06%\n",
      "Female-83.74%\n",
      "Female-71.20%\n",
      "Female-64.17%\n",
      "Female-77.18%\n",
      "Female-72.66%\n",
      "Female-77.13%\n",
      "Female-83.58%\n",
      "Female-73.69%\n",
      "Female-74.80%\n",
      "Female-75.58%\n",
      "Female-88.33%\n",
      "Female-83.10%\n",
      "Female-71.72%\n",
      "Female-87.42%\n",
      "Female-91.28%\n",
      "Female-70.56%\n",
      "Female-56.36%\n",
      "Female-73.00%\n",
      "Female-84.31%\n",
      "Female-65.79%\n",
      "Female-65.39%\n",
      "Female-71.86%\n",
      "Female-68.95%\n",
      "Female-76.02%\n",
      "Female-72.85%\n",
      "Female-85.29%\n",
      "Female-71.37%\n",
      "Female-70.66%\n",
      "Female-75.20%\n",
      "Female-71.69%\n",
      "Female-82.05%\n",
      "Female-81.54%\n",
      "Female-87.82%\n",
      "Female-64.35%\n",
      "Female-75.42%\n",
      "Female-68.19%\n",
      "Female-71.76%\n",
      "Female-83.87%\n",
      "Female-72.24%\n",
      "Female-50.26%\n",
      "Female-68.62%\n",
      "Female-79.74%\n",
      "Female-79.27%\n",
      "Female-69.77%\n",
      "Female-71.63%\n",
      "Female-81.02%\n",
      "Female-77.64%\n",
      "Female-58.82%\n",
      "Female-64.98%\n",
      "Female-78.20%\n",
      "Female-65.80%\n",
      "Female-79.73%\n",
      "Female-81.91%\n",
      "Female-80.83%\n",
      "Female-69.44%\n",
      "Female-78.16%\n",
      "Female-72.79%\n",
      "Female-83.66%\n",
      "Female-55.69%\n",
      "Female-71.94%\n",
      "Female-65.56%\n",
      "Female-71.26%\n",
      "Female-56.26%\n",
      "Female-67.92%\n",
      "Female-84.77%\n",
      "Female-74.50%\n",
      "Female-66.25%\n",
      "Female-74.74%\n",
      "Female-77.20%\n",
      "Female-81.30%\n",
      "Female-74.13%\n",
      "Female-82.15%\n",
      "Female-65.46%\n",
      "Female-83.53%\n",
      "Female-71.14%\n",
      "Female-72.74%\n",
      "Female-80.87%\n",
      "Female-76.74%\n",
      "Female-70.00%\n",
      "Female-64.42%\n",
      "Female-71.70%\n",
      "Female-88.58%\n",
      "Female-67.77%\n",
      "Female-81.30%\n",
      "Female-78.36%\n",
      "Female-66.67%\n",
      "Female-81.90%\n",
      "Female-87.33%\n",
      "Female-74.08%\n",
      "Female-66.63%\n",
      "Female-66.14%\n",
      "Female-90.53%\n",
      "Female-80.44%\n",
      "Female-76.55%\n",
      "Female-82.14%\n",
      "Female-78.95%\n",
      "Female-82.78%\n",
      "Female-87.15%\n",
      "Female-89.10%\n",
      "Female-80.88%\n",
      "Female-72.93%\n",
      "Female-65.99%\n",
      "Male-55.44%\n",
      "Female-71.89%\n",
      "Male-66.79%\n",
      "Female-63.91%\n",
      "Female-62.22%\n",
      "Female-62.52%\n",
      "Female-76.48%\n",
      "Female-81.14%\n",
      "Female-81.22%\n",
      "Female-83.24%\n",
      "Female-84.83%\n",
      "Female-76.71%\n",
      "Female-74.10%\n",
      "Female-57.20%\n",
      "Female-82.83%\n",
      "Female-80.05%\n",
      "Female-69.97%\n",
      "Female-75.61%\n",
      "Female-82.31%\n",
      "Female-87.83%\n",
      "Female-83.59%\n",
      "Female-80.54%\n",
      "Female-89.16%\n",
      "Female-63.14%\n",
      "Female-78.09%\n",
      "Female-85.15%\n",
      "Female-81.00%\n",
      "Female-81.23%\n",
      "Female-77.17%\n",
      "Female-54.01%\n",
      "Female-71.00%\n",
      "Female-82.26%\n",
      "Female-83.01%\n",
      "Female-76.71%\n",
      "Female-60.35%\n",
      "Female-78.33%\n",
      "Female-80.62%\n",
      "Female-78.93%\n",
      "Female-83.96%\n",
      "Female-82.83%\n",
      "Female-75.90%\n",
      "Female-88.15%\n",
      "Female-83.74%\n",
      "Female-90.83%\n",
      "Female-84.05%\n",
      "Female-84.43%\n",
      "Female-75.24%\n",
      "Female-79.96%\n",
      "Female-83.90%\n",
      "Female-82.88%\n",
      "Female-64.74%\n",
      "Female-76.86%\n",
      "Female-64.65%\n",
      "Female-86.30%\n",
      "Female-77.35%\n",
      "Female-73.82%\n",
      "Female-85.00%\n",
      "Female-87.85%\n",
      "Female-87.69%\n",
      "Female-83.71%\n",
      "Female-82.13%\n",
      "Female-78.17%\n",
      "Female-72.88%\n",
      "Female-83.84%\n",
      "Female-84.88%\n",
      "Female-90.17%\n",
      "Female-92.34%\n",
      "Female-86.48%\n",
      "Female-88.22%\n",
      "Female-85.23%\n",
      "Female-93.58%\n",
      "Female-81.58%\n",
      "Female-92.57%\n",
      "Female-78.54%\n",
      "Female-76.00%\n",
      "Female-68.68%\n",
      "Female-81.39%\n",
      "Female-74.76%\n",
      "Female-82.26%\n",
      "Female-83.90%\n",
      "Female-86.28%\n",
      "Female-87.96%\n",
      "Female-76.53%\n",
      "Female-79.72%\n",
      "Female-58.12%\n",
      "Female-86.20%\n",
      "Female-85.50%\n",
      "Female-71.63%\n",
      "Female-85.35%\n",
      "Female-80.13%\n",
      "Female-84.87%\n",
      "Female-75.50%\n",
      "Female-87.11%\n",
      "Female-80.69%\n",
      "Female-88.49%\n",
      "Female-88.50%\n",
      "Female-73.20%\n",
      "Female-87.92%\n",
      "Female-78.96%\n",
      "Female-86.09%\n",
      "Female-72.17%\n",
      "Female-81.69%\n",
      "Female-82.01%\n",
      "Female-80.28%\n",
      "Female-74.81%\n",
      "Female-81.99%\n",
      "Female-91.20%\n",
      "Female-78.32%\n",
      "Female-73.68%\n",
      "Female-77.94%\n",
      "Female-81.37%\n",
      "Female-75.61%\n",
      "Female-79.41%\n",
      "Female-81.57%\n",
      "Female-87.28%\n",
      "Female-83.71%\n",
      "Female-83.37%\n",
      "Female-80.28%\n",
      "Female-83.49%\n",
      "Female-83.91%\n",
      "Female-70.44%\n",
      "Female-92.39%\n",
      "Female-80.76%\n",
      "Female-92.28%\n",
      "Female-75.73%\n",
      "Female-77.57%\n",
      "Female-74.74%\n",
      "Female-86.81%\n",
      "Female-77.22%\n",
      "Female-82.87%\n",
      "Female-83.49%\n",
      "Female-83.83%\n",
      "Female-73.25%\n",
      "Female-93.02%\n",
      "Female-93.61%\n",
      "Female-93.05%\n",
      "Female-78.08%\n",
      "Female-83.73%\n",
      "Female-71.41%\n",
      "Female-85.19%\n",
      "Female-86.79%\n",
      "Female-79.08%\n",
      "Female-75.99%\n",
      "Male-53.89%\n",
      "Female-74.16%\n",
      "Female-64.91%\n",
      "Female-85.12%\n",
      "Female-71.77%\n",
      "Female-57.41%\n",
      "Female-78.66%\n",
      "Male-58.96%\n",
      "Female-63.11%\n",
      "Female-66.89%\n",
      "Female-52.98%\n",
      "Female-61.36%\n",
      "Male-76.70%\n",
      "Male-54.65%\n",
      "Male-53.49%\n",
      "Female-85.31%\n",
      "Female-75.15%\n",
      "Female-67.55%\n",
      "Male-59.65%\n",
      "Female-72.88%\n",
      "Male-70.76%\n",
      "Female-88.09%\n",
      "Female-86.05%\n",
      "Female-56.92%\n",
      "Female-93.62%\n",
      "Female-97.11%\n",
      "Female-91.55%\n",
      "Female-85.54%\n",
      "Male-50.40%\n",
      "Female-50.48%\n",
      "Female-80.65%\n",
      "Female-64.99%\n",
      "Female-83.46%\n",
      "Female-97.44%\n",
      "Female-93.55%\n",
      "Male-51.48%\n",
      "Male-52.72%\n",
      "Male-61.85%\n",
      "Male-69.86%\n",
      "Male-50.79%\n",
      "Male-63.08%\n",
      "Female-61.16%\n",
      "Male-80.21%\n",
      "Male-88.12%\n",
      "Male-73.23%\n",
      "Female-67.03%\n",
      "Female-54.06%\n",
      "Female-90.24%\n",
      "Female-62.80%\n",
      "Female-74.22%\n",
      "Female-77.59%\n",
      "Female-59.15%\n",
      "Female-82.67%\n",
      "Female-59.46%\n",
      "Male-77.60%\n",
      "Female-86.65%\n",
      "Female-75.20%\n",
      "Female-92.16%\n",
      "Female-56.89%\n",
      "Female-85.86%\n",
      "Female-86.56%\n",
      "Female-77.60%\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# The gender model architecture\n",
    "# https://drive.google.com/open?id=1W_moLzMlGiELyPxWiYQJ9KFaXroQ_NFQ\n",
    "GENDER_MODEL = 'weights/deploy_gender.prototxt'\n",
    "# The gender model pre-trained weights\n",
    "# https://drive.google.com/open?id=1AW3WduLk1haTVAxHOkVS_BEzel1WXQHP\n",
    "GENDER_PROTO = 'weights/gender_net.caffemodel'\n",
    "# Each Caffe Model impose the shape of the input image also image preprocessing is required like mean\n",
    "# substraction to eliminate the effect of illunination changes\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "# Represent the gender classes\n",
    "GENDER_LIST = ['Male', 'Female']\n",
    "# https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n",
    "FACE_PROTO = \"weights/deploy.prototxt.txt\"\n",
    "# https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180205_fp16/res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
    "FACE_MODEL = \"weights/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "\n",
    "# load face Caffe model\n",
    "face_net = cv2.dnn.readNetFromCaffe(FACE_PROTO, FACE_MODEL)\n",
    "# Load gender prediction model\n",
    "gender_net = cv2.dnn.readNetFromCaffe(GENDER_MODEL, GENDER_PROTO)\n",
    "\n",
    "# Initialize frame size\n",
    "frame_width = 1280\n",
    "frame_height = 720\n",
    "\n",
    "\n",
    "def get_faces(frame, confidence_threshold=0.5):\n",
    "    # convert the frame into a blob to be ready for NN input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104, 177.0, 123.0))\n",
    "    # set the image as input to the NN\n",
    "    face_net.setInput(blob)\n",
    "    # perform inference and get predictions\n",
    "    output = np.squeeze(face_net.forward())\n",
    "    # initialize the result list\n",
    "    faces = []\n",
    "    # Loop over the faces detected\n",
    "    for i in range(output.shape[0]):\n",
    "        confidence = output[i, 2]\n",
    "        if confidence > confidence_threshold:\n",
    "            box = output[i, 3:7] * \\\n",
    "                np.array([frame.shape[1], frame.shape[0],\n",
    "                         frame.shape[1], frame.shape[0]])\n",
    "            # convert to integers\n",
    "            start_x, start_y, end_x, end_y = box.astype(np.int)\n",
    "            # widen the box a little\n",
    "            start_x, start_y, end_x, end_y = start_x - \\\n",
    "                10, start_y - 10, end_x + 10, end_y + 10\n",
    "            start_x = 0 if start_x < 0 else start_x\n",
    "            start_y = 0 if start_y < 0 else start_y\n",
    "            end_x = 0 if end_x < 0 else end_x\n",
    "            end_y = 0 if end_y < 0 else end_y\n",
    "            # append to our list\n",
    "            faces.append((start_x, start_y, end_x, end_y))\n",
    "    return faces\n",
    "\n",
    "\n",
    "def get_optimal_font_scale(text, width):\n",
    "    \"\"\"Determine the optimal font scale based on the hosting frame width\"\"\"\n",
    "    for scale in reversed(range(0, 60, 1)):\n",
    "        textSize = cv2.getTextSize(text, fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=scale/10, thickness=1)\n",
    "        new_width = textSize[0][0]\n",
    "        if (new_width <= width):\n",
    "            return scale/10\n",
    "    return 1\n",
    "\n",
    "# from: https://stackoverflow.com/questions/44650888/resize-an-image-without-distortion-opencv\n",
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "    # resize the image\n",
    "    return cv2.resize(image, dim, interpolation = inter)\n",
    "\n",
    "\n",
    "def predict_gender():\n",
    "    \"\"\"Predict the gender of the faces showing in the image\"\"\"\n",
    "    # create a new cam object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        _, img = cap.read()\n",
    "        # resize the image, uncomment if you want to resize the image\n",
    "        # img = cv2.resize(img, (frame_width, frame_height))\n",
    "        # Take a copy of the initial image and resize it\n",
    "        frame = img.copy()\n",
    "        if frame.shape[1] > frame_width:\n",
    "            frame = image_resize(frame, width=frame_width)\n",
    "        # predict the faces\n",
    "        faces = get_faces(frame)\n",
    "        # Loop over the faces detected\n",
    "        # for idx, face in enumerate(faces):\n",
    "        for i, (start_x, start_y, end_x, end_y) in enumerate(faces):\n",
    "            face_img = frame[start_y: end_y, start_x: end_x]\n",
    "            # image --> Input image to preprocess before passing it through our dnn for classification.\n",
    "            # scale factor = After performing mean substraction we can optionally scale the image by some factor. (if 1 -> no scaling)\n",
    "            # size = The spatial size that the CNN expects. Options are = (224*224, 227*227 or 299*299)\n",
    "            # mean = mean substraction values to be substracted from every channel of the image.\n",
    "            # swapRB=OpenCV assumes images in BGR whereas the mean is supplied in RGB. To resolve this we set swapRB to True.\n",
    "            blob = cv2.dnn.blobFromImage(image=face_img, scalefactor=1.0, size=(\n",
    "                227, 227), mean=MODEL_MEAN_VALUES, swapRB=False, crop=False)\n",
    "            # Predict Gender\n",
    "            gender_net.setInput(blob)\n",
    "            gender_preds = gender_net.forward()\n",
    "            i = gender_preds[0].argmax()\n",
    "            gender = GENDER_LIST[i]\n",
    "            gender_confidence_score = gender_preds[0][i]\n",
    "            # Draw the box\n",
    "            label = \"{}-{:.2f}%\".format(gender, gender_confidence_score*100)\n",
    "            print(label)\n",
    "            yPos = start_y - 15\n",
    "            while yPos < 15:\n",
    "                yPos += 15\n",
    "            # get the font scale for this image size\n",
    "            optimal_font_scale = get_optimal_font_scale(label,((end_x-start_x)+25))\n",
    "            box_color = (255, 0, 0) if gender == \"Male\" else (147, 20, 255)\n",
    "            cv2.rectangle(frame, (start_x, start_y), (end_x, end_y), box_color, 2)\n",
    "            # Label processed image\n",
    "            cv2.putText(frame, label, (start_x, yPos),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, optimal_font_scale, box_color, 2)\n",
    "\n",
    "            # Display processed image\n",
    "        \n",
    "        # frame = cv2.resize(frame, (frame_height, frame_width))\n",
    "        cv2.imshow(\"Gender Estimator\", frame)\n",
    "        if cv2.waitKey(1) == ord(\"q\"):\n",
    "            break\n",
    "        # uncomment if you want to save the image\n",
    "        # cv2.imwrite(\"output.jpg\", frame)\n",
    "        # Cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    predict_gender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
